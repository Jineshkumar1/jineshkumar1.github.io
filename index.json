[{"categories":null,"contents":" Hello World A sample go program is show here.\npackage main import \u0026#34;fmt\u0026#34; func main() { message := greetMe(\u0026#34;world\u0026#34;) fmt.Println(message) } func greetMe(name string) string { return \u0026#34;Hello, \u0026#34; + name + \u0026#34;!\u0026#34; } Run the program as below:\n$ go run hello.go Variables Normal Declaration:\nvar msg string msg = \u0026#34;Hello\u0026#34; Shortcut:\nmsg := \u0026#34;Hello\u0026#34; Constants const Phi = 1.618 ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/notes/go/basic/introduction/","summary":" Hello World A sample go program is show here.\npackage main import \u0026#34;fmt\u0026#34; func main() { message := greetMe(\u0026#34;world\u0026#34;) fmt.Println(message) } func greetMe(name string) string { return \u0026#34;Hello, \u0026#34; + name + \u0026#34;!\u0026#34; } Run the program as below:\n$ go run hello.go Variables Normal Declaration:\nvar msg string msg = \u0026#34;Hello\u0026#34; Shortcut:\nmsg := \u0026#34;Hello\u0026#34; Constants const Phi = 1.618 ","tags":null,"title":"Introduction"},{"categories":null,"contents":" Strings str := \u0026#34;Hello\u0026#34; Multiline string\nstr := `Multiline string` Numbers Typical types\nnum := 3 // int num := 3. // float64 num := 3 + 4i // complex128 num := byte(\u0026#39;a\u0026#39;) // byte (alias for uint8) Other Types\nvar u uint = 7 // uint (unsigned) var p float32 = 22.7 // 32-bit float Arrays // var numbers [5]int numbers := [...]int{0, 0, 0, 0, 0} Pointers func main () { b := *getPointer() fmt.Println(\u0026#34;Value is\u0026#34;, b) func getPointer () (myPointer *int) { a := 234 return \u0026amp;a a := new(int) *a = 234 Pointers point to a memory location of a variable. Go is fully garbage-collected.\nType Conversion i := 2 f := float64(i) u := uint(i) Slice slice := []int{2, 3, 4} slice := []byte(\u0026#34;Hello\u0026#34;) ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/notes/go/basic/types/","summary":"Strings str := \u0026#34;Hello\u0026#34; Multiline string\nstr := `Multiline string` Numbers Typical types\nnum := 3 // int num := 3. // float64 num := 3 + 4i // complex128 num := byte(\u0026#39;a\u0026#39;) // byte (alias for uint8) Other Types\nvar u uint = 7 // uint (unsigned) var p float32 = 22.7 // 32-bit float Arrays // var numbers [5]int numbers := [...]int{0, 0, 0, 0, 0} Pointers func main () { b := *getPointer() fmt.","tags":null,"title":"Basic Types"},{"categories":null,"contents":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) Switch switch day { case \u0026#34;sunday\u0026#34;: // cases don\u0026#39;t \u0026#34;fall through\u0026#34; by default! fallthrough case \u0026#34;saturday\u0026#34;: rest() default: work() } Loop for count := 0; count \u0026lt;= 10; count++ { fmt.Println(\u0026#34;My counter is at\u0026#34;, count) } entry := []string{\u0026#34;Jack\u0026#34;,\u0026#34;John\u0026#34;,\u0026#34;Jones\u0026#34;} for i, val := range entry { fmt.Printf(\u0026#34;At position %d, the character %s is present\\n\u0026#34;, i, val) n := 0 x := 42 for n != x { n := guess() } ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/notes/go/basic/flow-control/","summary":"Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) Switch switch day { case \u0026#34;sunday\u0026#34;: // cases don\u0026#39;t \u0026#34;fall through\u0026#34; by default! fallthrough case \u0026#34;saturday\u0026#34;: rest() default: work() } Loop for count := 0; count \u0026lt;= 10; count++ { fmt.Println(\u0026#34;My counter is at\u0026#34;, count) } entry := []string{\u0026#34;Jack\u0026#34;,\u0026#34;John\u0026#34;,\u0026#34;Jones\u0026#34;} for i, val := range entry { fmt.","tags":null,"title":"Flow Control"},{"categories":null,"contents":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/notes/go/advanced/files/","summary":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) ","tags":null,"title":"File Manipulation"},{"categories":null,"contents":" Variable NAME=\u0026#34;John\u0026#34; echo $NAME echo \u0026#34;$NAME\u0026#34; echo \u0026#34;${NAME} Condition if [[ -z \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is empty\u0026#34; elif [[ -n \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is not empty\u0026#34; fi ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/notes/bash/basic/","summary":" Variable NAME=\u0026#34;John\u0026#34; echo $NAME echo \u0026#34;$NAME\u0026#34; echo \u0026#34;${NAME} Condition if [[ -z \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is empty\u0026#34; elif [[ -n \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is not empty\u0026#34; fi ","tags":null,"title":"Bash Variables"},{"categories":["Basic"],"contents":"Azure AZCopy Migration from AWS S3 Bucket to Azure Storage Components we will use,\nInstall AZCopy Data on AWS S3 bucket which will be the source. Azure Storage Account and Container created there which will be Target. Access and Authorization with Azure and AWS (Using SAS Token and SAK and SKI) Don\u0026rsquo;t worry if you are new to this, I have provided Step by Step Guide on how to use all the above components and AZCopy to migrate Data from AWS S3 to Azure Container.\nFollow along,\nDownload Azcopy from, https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10\nWe have to Install AZCopy and start azcopy.exe using CLI as per, Permission to access AWS S3 Bucket and Azure Storage Account On AWS Side, We need AWS Access Key ID and AWS Secret Access Key and set it in Azcopy CMD\nAWS Console \u0026gt; IAM \u0026gt; Users \u0026gt; Select Admin User \u0026gt; Security Credential \u0026gt; Create Access Key\nDownload and Set it to our Azcopy CMD Console as per below, (For Security purposes, I have dash out SAK and SKI)\nOn Azure side, We need Azure Target Storage Account SAS Token\nAzure Portal \u0026gt; Target Storage Account \u0026gt; Shared Access Signature \u0026gt; Generate \u0026gt; Copy the SAS Token As per shown in below screenshot\nNote : I could have create a SAS Token at Container Level and it would have work as well but to generalize for this blog, I did it at storage account level SAS Token.\nFinal Command on AZCopy CMD We have\nAWS : S3 Bucket Name , Set Access Key ID and Secret Access Key\nAzure : Destination Storage Account Name , Target Container Name, Copied SAS Token Now, what we need to run ,\nazcopy cp \u0026#34;https://s3.amazonaws.com/[bucket]\u0026#34; \u0026#34;https://[destaccount].blob.core.windows.net/[container]?[SAS]\u0026#34; --recursive=true Mine looked like,\nC:\\azurelabs\\azcopy_windows_amd64_10.11.0\u0026gt;azcopy cp \u0026#34;https://s3.amazonaws.com/**migrationworkload**\u0026#34; \u0026#34;https://**migrationtargetazure**.blob.core.windows.net/**azcopytarget**?sastoken--XXXX--sastoken\u0026#34; --recursive=true Job Summary shows,\nHow many Files got transferred\nand Final Job Status : Completed.\nAnd There you go, Files appeared On Target Azure Container.\nNothing less than Magic.!!!!\nNote: AZCopy is a powerful tool. It can be used for clients who wants On-Prem Data migration over to Azure Blob Storage as well. The command looks similar to this,\nazcopy copy \u0026#34;\u0026lt;local-folder-path\u0026gt;\u0026#34; \u0026#34;https://\u0026lt;storage-account-name\u0026gt;.\u0026lt;blob or dfs\u0026gt;.core.windows.net/\u0026lt;container-name\u0026gt;\u0026#34; --recursive=true Feel free to try and Let me know if you have any questions or improvements I can provide.\nThanks for the read.\nFollow for more Awesome Azure and AWS Content.\nRegards, Jineshkumar Patel\n","date":"June 8, 2022","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/posts/azure/azure-blogs/azcopy-migration/azure-azcopy-migration-from-aws-s3-bucket-to-azure-storage/","summary":"Azure AZCopy Migration from AWS S3 Bucket to Azure Storage Components we will use,\nInstall AZCopy Data on AWS S3 bucket which will be the source. Azure Storage Account and Container created there which will be Target. Access and Authorization with Azure and AWS (Using SAS Token and SAK and SKI) Don\u0026rsquo;t worry if you are new to this, I have provided Step by Step Guide on how to use all the above components and AZCopy to migrate Data from AWS S3 to Azure Container.","tags":["Azure","Cloud","Azure Migration","Data","Azure Storage"],"title":"Data Migration from AWS S3 Bucket to Azure Storage"},{"categories":["Basic"],"contents":"Windows Subsystem for Linux USL2 How-To Guide of Installation.(Ubuntu)\n**WSL2 **was announced at Microsoft Build 2019. WSL2 features a Linux kernel running inside Windows 10 and is built on the core technology of Hyper-V to provide better Linux application support and improved file performance. Transitioning to WSL2 is seamless.\nThis is a Step by Step guide on how to install Ubuntu on your Windows 10 Machine.\nTurn On Windows Subsystem for Linux on Windows OS After WSL 2.0 gets Enabled. It will require a Reboot of Windows OS. Linux Distro is not installed. Tested by running Bash. Open Microsoft Store \u0026gt; Download and Install Ubuntu. The following links will open the Microsoft store page for each distribution:\nUbuntu 18.04 LTS , 20.04 LTS\nopenSUSE Leap 15.1\nSUSE Linux Enterprise Server 12 SP5\nKali Linux\nDebian GNU/Linux\nFedora Remix for WSL\nPengwin Enterprise\nAlpine WSL After Install \u0026gt; Launch it. It will take few minutes to unzip all files locally. Asked for a new Username and Password. And Ubuntu Successfully got installed. Checkout ls-al command to see the filesystem it has You can mount local Disks (C:, D:, G:\\ Drives) onto this Linux Distro. Some other commands to test and learn Ubuntu which is locally deployed within your Windows OS. Learn UBUNTU for Testing/Learning or Project Purposes,\n40+ most used Ubuntu 20.04 Commands\nHope you like my short Step by step guide on How to Install Windows Subsystem for Linux Tutorial where we installed Ubuntu Linux Distro locally on windows 10 OS.\nFeel free to follow along and Let me know if you have any questions or improvements I can provide.\nI hope you like the Blog: \u0026ldquo;WSL : Step by step guide for installation of Ubuntu on local Windows OS\u0026rdquo;.\nThanks for the read. Follow for more Awesome Azure and AWS Content.\nRegards,\nJineshkumar Patel\n","date":"June 8, 2022","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/posts/wsl2/windows-subsystem-for-linux-usl2/","summary":"Windows Subsystem for Linux USL2 How-To Guide of Installation.(Ubuntu)\n**WSL2 **was announced at Microsoft Build 2019. WSL2 features a Linux kernel running inside Windows 10 and is built on the core technology of Hyper-V to provide better Linux application support and improved file performance. Transitioning to WSL2 is seamless.\nThis is a Step by Step guide on how to install Ubuntu on your Windows 10 Machine.\nTurn On Windows Subsystem for Linux on Windows OS After WSL 2.","tags":["Basic","WSL2"],"title":"Windows SubSystem for Linux"},{"categories":["Basic"],"contents":"What is Azure Storage ? Azure Storage is a Microsoft-managed service providing cloud storage that is highly available, secure, durable, scalable, and redundant. Azure Storage includes Azure Blobs (objects), Azure Data Lake Storage Gen2, Azure Files, Azure Queues, and Azure Tables.\nAzure Storage Account is what we create on azure to store any kind of Data (Blob Objects, Files, Table, Queue, )\nThe storage account provides a unique namespace (URL) for your Azure Storage data that is accessible from anywhere in the world over HTTP or HTTPS.\nSo mainly 5 storage services :\nBLOB , QUEUE, FILE , VM Disks and TABLE.\nEach has its own uniqueness of usage scenario as per the requirement. Azure Storage consists out of multiple services that are each optimized for a certain usage scenario. They are described in this post, and here is a summary of them:\nAzure Blob Storage\nUseful for storing files, small and large, like audio, video or VHD files\nAzure Queue Storage\nMeant for storing small messages that are picked up by other applications. Queue Storage can help to decouple your applications\nAzure File Storage\nBased on the SMB protocol, File Storage is meant to be mounted as a disk in a VM. It is very useful to use for lifting and shifting applications into the cloud\nAzure Disk Storage\nDisk Storage is optimized for high I/O operations and can be used as a hard disk for a VM, for a server.\nAzure Table Storage\nAzure Table storage is a service that stores structured NoSQL data in the cloud, providing a key/attribute store with a schemaless design. Because Table storage is schemaless. Access to Table storage data is fast and cost-effective for many types of applications and is typically lower in cost than traditional SQL for similar volumes of data.\nStorage Account Types General-purpose v2 (GPv2) -\nDefault, Basic storage account type for blobs, files, queues, and tables. Azure Recommended for most scenarios using Azure Storage + Access Tiers (Cool , Hot, Archive).\nPremium (Block Blobs, NFS File Shares, Page Blobs) -\nRecommended for scenarios with high transactions rates, or scenarios that use smaller objects or require consistently low storage latency\n**Now, what is Hot/Cold/Archive Blob Tiering in Azure ? ** There are three tiers of blob storage in Azure:\nHot\nThe cheapest to access, but the most expensive to store.\nHighly vailable and most durable storage.\nCool\nStill low latency, but cheap per GB capacity at higher access rate and stored for at least 30 days. Data in the cool access tier can tolerate slightly lower availability, but still requires high durability, retrieval latency, and throughput characteristics like hot data. For cool data, a slightly lower availability service-level agreement (SLA) and higher access costs compared to hot data are acceptable trade-offs for lower storage costs.\nArchive\nExtremely cheap per GB storage (~$2.05 per TB per month). The cheapest per GB capacity but it takes up to 15 hours to move a blob back to cool/hot where it can be accessed again.\nNow, Storage Account Endpoints, A storage account provides a unique namespace in Azure for your data. Every object that you store in Azure Storage has an address that includes your unique account name. The combination of the account name and the Azure Storage service endpoint forms the endpoints for your storage account.\nThe following table lists the format of the endpoint for each of the Azure Storage services.\nStorage service\tEndpoint Blob storage\thttps://\u0026lt;storage-account\u0026gt;.blob.core.windows.net Azure Files\thttps://\u0026lt;storage-account\u0026gt;.file.core.windows.net Queue storage\thttps://\u0026lt;storage-account\u0026gt;.queue.core.windows.net Table storage\thttps://\u0026lt;storage-account\u0026gt;.table.core.windows.net Storage account billing Azure Storage bills based on your storage account usage. All objects in a storage account are billed together as a group. Storage costs are calculated according to the following factors:\nRegion refers to the geographical region in which your account is based.\nAccount type refers to the type of storage account you\u0026rsquo;re using.\nAccess tier refers to the data usage pattern you\u0026rsquo;ve specified for your general-purpose v2 or Blob storage account.\nCapacity refers to how much of your storage account allotment you\u0026rsquo;re using to store data.\nRedundancy determines how many copies of your data are maintained at one time, and in what locations.\nTransactions refer to all read and write operations to Azure Storage.\nData egress refers to any data transferred out of an Azure region. When the data in your storage account is accessed by an application that isn\u0026rsquo;t running in the same region, you\u0026rsquo;re charged for data egress.\nAzure Pricing Calculator for Estimates How to Create Storage Account ? We need to follow below steps to “Create storage account”\nGo to azure portal and Login \u0026gt; [https://portal.azure.com](https://portal.azure.com) Search \u0026quot;Storage Account\u0026quot; \u0026gt; Select \u0026gt; Create Select Resource Group/Create New Insert Storage Account Name : It has to be lowercase and unique\nSelect Performance Tier \u0026gt; Standard (GPv2) or Premium\nSelect Redundancy: Geo-Redundant (GRS) / LRS / ZRS / GZRS\nMore on Redundancy\nNext : \u0026ldquo;Advanced\u0026rdquo; for Security permissions(REST API, Encryption)\nand\nAccess to storage (Public Access , Storage Account Key Access)\nand\nSelect Minimum TLS (Transport Layer Security) version needed for request to Storage Account.\nand\nAccess Tier : Hot / Cool Next : Networking Connectivity Methods: Public Endpoint (All Networks) OR Public Selected Networks OR Private Endpoint. And Network Routing : Microsoft Network Routing OR Internet Routing. Next: Data Protection\nEnable Point-in-Time Recovery , SOFT DELETE and ENABLE VERSIONING , Enable Blob Change Feed. Next: Tags\n(It is Best Practice to always Add Tags for Organize and Billing Purposes)\nReview + Create\nIts will show you the Summary of your Configuration and Validation Result.\nNOTE: Notice at the bottom, \u0026ldquo;Download Template for Automation\u0026rdquo; is the JSON Template to use your selected configuration as a ARM Template to Create for future need just by deploying that Template.\nCreate I hope you enjoy the walkthrough and my All about** Azure Storage Account**\nFeel free to try and Let me know if you have any questions or improvements I can provide.\nThanks for the read. Follow for more Awesome Azure and AWS Content.\nRegards,\n**Jineshkumar Patel **\n","date":"June 8, 2020","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/posts/azure/azure-blogs/azure-storage-account/azure-storage-account/","summary":"What is Azure Storage ? Azure Storage is a Microsoft-managed service providing cloud storage that is highly available, secure, durable, scalable, and redundant. Azure Storage includes Azure Blobs (objects), Azure Data Lake Storage Gen2, Azure Files, Azure Queues, and Azure Tables.\nAzure Storage Account is what we create on azure to store any kind of Data (Blob Objects, Files, Table, Queue, )\nThe storage account provides a unique namespace (URL) for your Azure Storage data that is accessible from anywhere in the world over HTTP or HTTPS.","tags":["Azure","Cloud","Storage","Azure Storage"],"title":"All about Azure Storage"},{"categories":["Basic"],"contents":"Create an Azure function app in the Azure portal. Azure Functions (FaaS) is simply a platform to upload the application code, run, and manage the application without having to think about setting up any servers.\nServerless compute can be thought of as a \u0026ldquo;function as a service\u0026rdquo; (FaaS), or a microservice that is hosted on a cloud platform. Your business logic runs as functions and you don\u0026rsquo;t have to manually provision or scale infrastructure. Serverless computing helps solve the allocation problem by scaling up or down automatically, and you\u0026rsquo;re only billed when your function is processing work.\nThe two most common approaches are Azure Logic Apps and Azure Functions, which we\u0026rsquo;ll focus on this blog.\nAzure Functions is the** function-as-a-service (FaaS)** offering from Azure Serverless the equivalent of AWS Lambda from Amazon.\nWhat is Azure Functions? Azure Functions is a serverless application platform. It enables developers to host business logic that can be executed without provisioning infrastructure. Functions provides intrinsic scalability and you are charged only for the resources used. You can write your function code in the language of your choice, including C#, F#, JavaScript, Python, and PowerShell Core. Support for package managers like NuGet and NPM is also included, so you can use popular libraries in your business logic.\nBenefits of a serverless compute solution Avoids over-allocation of infrastructure Stateless logic Event / Trigger Driven. Can be converted into Traditional Compute Environment. Create a function app in the Azure portal From Azure Portal \u0026gt; Azure Services \u0026gt; Create a Resource \u0026gt; Compute \u0026gt; Select Function App And fill out all the Basics Tab\u0026rsquo;s information as follows, Publish\tCode\nRuntime stack\tNode.js Next \u0026gt;Hosting : Select Storage Account, Service Plan : Consumption (Serverless) Note : Choose the Consumption plan when using the Azure serverless application platform. This plan provides automatic scaling and bills you when your functions are running. The Consumption plan comes with a configurable timeout period for the execution of a function. By default, it\u0026rsquo;s five (5) minutes, but may be configured to have a timeout as long as 10 minutes.\nNo to Monitoring and No Tags. (Just for this example purposes)\nReview + Create and Create\nResult\nNow that we have a function app created, let\u0026rsquo;s look at how to build, configure, and execute a function. We need to Run our code and Add logic to the function app on-demand with Azure Functions Functions are event driven (Triggers), which means they run in response to an event.\nYou must configure a function with exactly one trigger.\nTriggers can be any of the following services\nBlob Storage Queue Storage Event Grid HTTP Microsoft Graph Events Azure Cosmos DB Service Bus Timer **Little bit about bindings **\nBindings are a declarative way to connect data and services to your function. Bindings know how to talk to different services, which means you don\u0026rsquo;t have to write code in your function to connect to data sources and manage connections.\nEach function can have zero or more bindings to manage the input and output data processed by the function.\nDefining a sample binding for an Example, \u0026quot; Let\u0026rsquo;s say we want to write a new row to Azure Table storage whenever a new message appears in Azure Queue Storage. This scenario can be implemented using an Azure Queue Storage trigger and an Azure Table storage output binding.\u0026quot;\nJSON\n{ \u0026#34;bindings\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;order\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;queueTrigger\u0026#34;, \u0026#34;direction\u0026#34;: \u0026#34;in\u0026#34;, \u0026#34;queueName\u0026#34;: \u0026#34;myqueue-items\u0026#34;, \u0026#34;connection\u0026#34;: \u0026#34;MY_STORAGE_ACCT_APP_SETTING\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;$return\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;table\u0026#34;, \u0026#34;direction\u0026#34;: \u0026#34;out\u0026#34;, \u0026#34;tableName\u0026#34;: \u0026#34;outTable\u0026#34;, \u0026#34;connection\u0026#34;: \u0026#34;MY_TABLE_STORAGE_ACCT_APP_SETTING\u0026#34; } ] } This example is a simple illustration of how we configure bindings for a function. We could change the output to be an email using a SendGrid binding, or put an event onto a Service Bus to notify some other component in our architecture, or even have multiple output bindings to push data to various services.\nCreate a function in the Azure portal in your Function App,\nAzure provides several pre-made function templates for common scenarios: Selecting a template from the Add function pane provides easy access to the most common templates. as per,\nScreenshot shows I selected HTTP Trigger 1 function Navigate to your function and files : HTTPTrigger1 | Code + Test \u0026gt; Test/Run \u0026gt; function.json file As per,\nTest in the Azure portal The portal also provides a convenient way to test your functions. As previously described, after selecting Test/Run from the top menu bar, on the right side of the code box, a tabbed Test pane appears. Here, you can provide parameters to test the HTTP request. When you select Run in this pane, the results appear in the Output tab, along with a status code in the Logs pane.\nAdd logic to the function app Example : Add the logic for the temperature number. Specifically, we\u0026rsquo;re going to receive data from an HTTP request.\nFirst, we need to define some requirements for our logic:\nTemperatures between 0-25 degrees should be flagged as OK. Temperatures between 26-50 degrees should be flagged as CAUTION. Temperatures above 50 degrees should be flagged as DANGER.\nReplace Index.js file content with following for our js logic JS code, as per,\nmodule.exports = function (context, req) { context.log(\u0026#39;Drive Gear Temperature Service triggered\u0026#39;); if (req.body \u0026amp;\u0026amp; req.body.readings) { req.body.readings.forEach(function(reading) { if(reading.temperature\u0026lt;=25) { reading.status = \u0026#39;OK\u0026#39;; } else if (reading.temperature\u0026lt;=50) { reading.status = \u0026#39;CAUTION\u0026#39;; } else { reading.status = \u0026#39;DANGER\u0026#39; } context.log(\u0026#39;Reading is \u0026#39; + reading.status); }); context.res = { // status: 200, /* Defaults to 200 */ body: { \u0026#34;readings\u0026#34;: req.body.readings } }; } else { context.res = { status: 400, body: \u0026#34;Please send an array of readings in the request body\u0026#34; }; } context.done(); }; The logic we added is straightforward. We iterate over the array of readings and check the temperature field. Depending on the value of that field, we set a status of OK, CAUTION, or DANGER. We then send back the array of readings with a status field added to each entry.\nTest our business logic In this case, we\u0026rsquo;re going to use the Test pane in the portal to test our function.\nIn the Test pane, under the Input tab, paste the sample request into the request Body text box.\n{ \u0026#34;readings\u0026#34;: [ { \u0026#34;driveGearId\u0026#34;: 1, \u0026#34;timestamp\u0026#34;: 1534263995, \u0026#34;temperature\u0026#34;: 23 }, { \u0026#34;driveGearId\u0026#34;: 3, \u0026#34;timestamp\u0026#34;: 1534264048, \u0026#34;temperature\u0026#34;: 45 }, { \u0026#34;driveGearId\u0026#34;: 18, \u0026#34;timestamp\u0026#34;: 1534264050, \u0026#34;temperature\u0026#34;: 55 } ] } like this,\n**Select Run ** and view the response under the Output tab.\nResource to know more in detail,\nCreate serverless logic with Azure Functions\nFeel free to try and Let me know if you have any questions or improvements I can provide.\nI hope you like the Blog: Introduction to ARM Templates. Thanks for the read. Follow for more Awesome Azure and AWS Content.\nRegards,\nJineshkumar Patel\n","date":"June 8, 2020","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/posts/azure/azure-blogs/azure-function-app/create-an-azure-function-app-in-the-azure-portal./","summary":"Create an Azure function app in the Azure portal. Azure Functions (FaaS) is simply a platform to upload the application code, run, and manage the application without having to think about setting up any servers.\nServerless compute can be thought of as a \u0026ldquo;function as a service\u0026rdquo; (FaaS), or a microservice that is hosted on a cloud platform. Your business logic runs as functions and you don\u0026rsquo;t have to manually provision or scale infrastructure.","tags":["Azure","Cloud","Azure Function"],"title":"Azure function APP with Demo"},{"categories":["Basic"],"contents":"How to Deploy DockerHub Images onto Docker Container using Azure Container Instance Deploy a docker container using container instance from the Azure portal Resource group: Create new \u0026gt; ContainerRG Container name: \u0026ldquo;securedockercontainer\u0026rdquo; Image source: Docker Hub or Other Registry , Image Type: Public Docker Hub Image: glennc/fancypants:latest OS Type: Linux , 1vCPU, 1.5 GBs This image is created by SCOTT HANSELMAN and Glenn C. Who Created .NET WebApp and put it as Public Docker Hub registry here which can be used by Developers to Learn and Test Case purposes. Thanks Scott. All Credits to you.\nINFO: Docker HUB is a service provided by Docker for finding and sharing container images with your team. It is the world’s largest repository of container images with an array of content sources including container community developers, open source projects and independent software vendors (ISV) building and distributing their code in containers.\nFirst, On Azure Portal \u0026gt; Search **Container Instance **\u0026gt; Create\nFill out required fields notated with rectangle boxes. Next is Networking Type which will be Public (\u0026lsquo;Public\u0026rsquo; will create a public IP address for your container instance.) And also you put \u0026ldquo;DNS Label\u0026rdquo; which will be your FQDN Endpoint for your WebApp.\nNext is Advanced where you select \u0026ldquo;Restart Policy\u0026rdquo; (Determines when your container should restart. You can choose to always restart the container regardless of how it stopped, to only restart if it failed to exit successfully or to never restart.) Next Tags and \u0026ldquo;Review + Create\u0026rdquo; \u0026gt; Create\nOnce Created \u0026gt; Go to Resources All the Important Tabs Highlighted or Boxed. Check it all out.\nNote: Container is in Running State.\nNotice it gives us an overview of Resources Monitored of Container\u0026rsquo;s CPU, Memory, and Network right there.\nThere are Public IP Address and FQDN given which can be used to access your Container Deployed .NET WebAPP.\nLet\u0026rsquo;s try to Copy IP Addresse and Open it on a Browser and then Copy FQDN (Fully Qualified Domain Name) for our Containerized app.\nBoth will open our .Net WebApp running from \u0026ldquo;securedockercontainer\u0026rdquo; deployed using Azure Container Instance. **EasyPeezy !! ** Right ?\nOn Azure Portal Container Instance Menu, You can Monitor Resources: CPU, Memory, Network from Overview Pan.\nAnd View Events, Logs and Connect from Azure Container Instance Tabs\u0026gt; Settings \u0026gt; Containers. Now that we have an .net Web APP running in Docker Container deployed using Azure Container Instance, We will discuss how we can Secure it in one of the upcoming Blogs. Hope you like this blog and Thank you for the read. I appreciate your time. Let me know if you have any questions or queries during following along with the steps.\nFollow for more Azure and AWS Content.\nRegards,\nJineshkumar Patel\n","date":"June 8, 2020","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/posts/azure/azure-blogs/deploy-docket-image/deploy-dockerhub-images-onto-docker-container-using-azure-container-instance/","summary":"How to Deploy DockerHub Images onto Docker Container using Azure Container Instance Deploy a docker container using container instance from the Azure portal Resource group: Create new \u0026gt; ContainerRG Container name: \u0026ldquo;securedockercontainer\u0026rdquo; Image source: Docker Hub or Other Registry , Image Type: Public Docker Hub Image: glennc/fancypants:latest OS Type: Linux , 1vCPU, 1.5 GBs This image is created by SCOTT HANSELMAN and Glenn C. Who Created .NET WebApp and put it as Public Docker Hub registry here which can be used by Developers to Learn and Test Case purposes.","tags":["Azure","Cloud","Docker","ACI"],"title":"Deploy DockerHub Images onto Docker Container using Azure Container Instance"},{"categories":["Basic"],"contents":"Deploy \u0026ldquo;Voting App\u0026rdquo; using Azure Kubernetes Cluster Service Azure Kubernetes Service (AKS) is a managed Kubernetes service that lets you quickly deploy and manage clusters.\nThis is a Tutorial on how to deploy a container-based application (Voting-App) using Azure Kubernetes Cluster service.\nFirst, Few very basics about Kubernetes, Kubernetes is a production-ready, portable, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation.\nBasically, Kubernetes is a container orchestration tool When you deploy Kubernetes, you get a cluster. This Cluster has MASTER NODE and WORKER NODEs.\nThe abbreviation K8s is derived by replacing the eight letters of “ubernete” with the digit 8.\nTo understand Kubernetes Architecture overview, I post some Images and their Source which are good place to start learning about Kubernetes if you are new to k8s.\nNote : This is just overview understand this Application Deployment for this Blog. I have plans to provide more in-depth Kubernetes tutorials coming soon. Consider Following my Blogs.Thx.\nImage Source : https://kubernetes.io/docs/tutorials/kubernetes-basics/\nImage Source : https://www.edureka.co/blog/kubernetes-tutorial/\nImage source : https://collabnix.com/5-minutes-to-kubernetes-architecture/\nLet\u0026rsquo;s get started with our Deployment, To-do List :\nDeploy an AKS(Azure K8s Service) cluster using Azure Portal. Run a multi-container application with a web front-end and a Redis instance in the cluster.(Voting App. taken from Open Source Github Repo: dockersamples/ example-voting-app ) Create Azure Kubernetes Cluster Service using Azure Portal On Azure Portal \u0026gt; Search Kubernetes Service \u0026gt; Create k8s Cluster and It will ask for Product details like Resource Group Name, Cluster Name, Region, Availability Zones, Kubernetes Version, Node Size, Scale Method(AutoScaling), Node Count Range (1-2).\nConfigure those as you want then Review + Create As per,\nNote: Primary node pool The number and size of nodes in the primary node pool in your cluster. For production workloads, at least 3 nodes are recommended for resiliency. For development or test workloads, only one node is required.\nValidation passed \u0026gt; Hit Create.\nAnd We have successfully deployed Azure Kubernetes Cluster with 2 nodes on which we can deploy our Container-based Voting Application. Get into ** Azure Cloud Shell** to run our k8s Cluster and Deploy our app. Go to Resource once deployment successful notification popup.\nOur K8s Cluster (VotingAppCluster) Overview page,\nOpen Azure CLI from the Azure Portal button from top-right corner. Bash Azure Cloud Shell will pop from the bottom,\nNote : When you first-time open the Cloud Shell, you will find that it requires you to create a Storage account. The reason for that Storage Account is to persist the scripts, keys, etc that you\u0026rsquo;ll use over and over as you interact with your resources\nGet access credentials for a managed Kubernetes cluster. az aks get-credentials --resource-group YourResourceGroupNAME --name YourAKSClusterNAME Verify the connection to your cluster using the \u0026ldquo;kubectl get nodes\u0026rdquo; This command returns a list of the cluster nodes. (We have 2) Run the Application The sample Azure Vote Python applications.\nA Redis instance.\nFirst Create a file named votingapp.yaml\nI used nano editor to create, Edit, and save .yaml file.\n-paste below content into the nano editor for votingapp.yaml file\nrun \u0026gt; nano votingapp.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: azure-vote-back spec: replicas: 1 selector: matchLabels: app: azure-vote-back template: metadata: labels: app: azure-vote-back spec: nodeSelector: \u0026#34;kubernetes.io/os\u0026#34;: linux containers: - name: azure-vote-back image: mcr.microsoft.com/oss/bitnami/redis:6.0.8 env: - name: ALLOW_EMPTY_PASSWORD value: \u0026#34;yes\u0026#34; resources: requests: cpu: 100m memory: 128Mi limits: cpu: 250m memory: 256Mi ports: - containerPort: 6379 name: redis --- apiVersion: v1 kind: Service metadata: name: azure-vote-back spec: ports: - port: 6379 selector: app: azure-vote-back --- apiVersion: apps/v1 kind: Deployment metadata: name: azure-vote-front spec: replicas: 1 selector: matchLabels: app: azure-vote-front template: metadata: labels: app: azure-vote-front spec: nodeSelector: \u0026#34;kubernetes.io/os\u0026#34;: linux containers: - name: azure-vote-front image: mcr.microsoft.com/azuredocs/azure-vote-front:v1 resources: requests: cpu: 100m memory: 128Mi limits: cpu: 250m memory: 256Mi ports: - containerPort: 80 env: - name: REDIS value: \u0026#34;azure-vote-back\u0026#34; --- apiVersion: v1 kind: Service metadata: name: azure-vote-front spec: type: LoadBalancer ports: - port: 80 selector: app: azure-vote-front Paste the Above Copied content in there. Cnt + 0 to save file [Wrote 85 lines] in my .yaml\nand Cnt + z to exit out. Deploy the application using the \u0026ldquo;kubectl apply\u0026rdquo; specify the name of your YAML manifest file. as per, kubectl apply -f votingapp.yaml Our Voting App is deployed successfully when you get that \u0026ldquo;azure-vote-back\u0026rdquo; and \u0026ldquo;azure-vote-front\u0026rdquo; get created. Now it\u0026rsquo;s about time to TEST Out the App. We need the Public IP Address. To get our deployment has created an external IP, run below command, kubectl get service azure-vote-front --watch This shows our Application\u0026rsquo;s external Public IP address.\nMy App\u0026rsquo;s External-IP = 52.190.47.180\nCopy External-IP (52.190.47.180 ) and paste it on your Browser You got that. This App is deployed on 2 Cluster Nodes with high availibility and resiliency.\nNodes and Cluster Monitoring for our Voting App Under AKS \u0026gt; Cluster Insight / Monitor Tabs respectively. ** Success. **\nAzure Kubernetes Cluster Service, It is a fully managed Kubernetes Cluster Service. Deploy a managed Kubernetes cluster in Azure. – Reduces the complexity and operation overhead of managing K8s by offloading much of that responsibility to Azure Azure AKS Cluster Handles critical operations tasks like load balancing, scaling, health monitoring, and maintenance for you. I am on my continuous learning quest and Kubernetes is in the top list. I apologize if I may have missed some key details or explained less on some commands performed above.\nI will be coming up with more in-depth tutorials and blogs about Kubernetes and DevOps in future content.\nHope you have followed along and like the Blog about\nDeploy \u0026ldquo;Voting App\u0026rdquo; using Azure Kubernetes Cluster Service\nLet me know if you have any questions or suggestions to improve my knowledge.\nThanks for your time.\nFollow for more Awesome Azure and AWS Content.\nRegards,\nJineshkumar Patel\n","date":"June 8, 2020","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/posts/azure/azure-blogs/voting-app-in-aks/deploy-voting-app-using-azure-kubernetes-cluster-service/","summary":"Deploy \u0026ldquo;Voting App\u0026rdquo; using Azure Kubernetes Cluster Service Azure Kubernetes Service (AKS) is a managed Kubernetes service that lets you quickly deploy and manage clusters.\nThis is a Tutorial on how to deploy a container-based application (Voting-App) using Azure Kubernetes Cluster service.\nFirst, Few very basics about Kubernetes, Kubernetes is a production-ready, portable, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation.\nBasically, Kubernetes is a container orchestration tool When you deploy Kubernetes, you get a cluster.","tags":["Azure","Cloud","AKS","Azure Kubernetes"],"title":"Deploy Voting App using AKS"},{"categories":["Basic"],"contents":"How to host your static website using Apache2 on a Linux machine deployed on Azure. In this quick tutorial, We are going to learn Azure Linux VM Configuration, Few Linux VM Commands, and Apache Web Server Install and display our own Webpage on a Public IP address from anywhere.\nDeployment Components : Linux VM : Ubuntu Server 20.04 LTS Compute Resources on : **Azure ** Web Server : **Apache2 ** Web Front End : HTML Deployment Steps 1st Step: Deploy Azure Compute Resource : Linux VM (Ubuntu 20.04) -\u0026gt; Login to Azure Portal \u0026gt; Create a \u0026ldquo;Compute\u0026rdquo; Resource\n-\u0026gt; Select Ubuntu Server 20.04 LTS \u0026gt; Create\n-\u0026gt; Make sure you have selected\nUsername: azureuser and\nSSH Key Pair Name: UbuntuVM_key\nClick **Review + Create ** -\u0026gt; Download Private Key and Create Resource\n-\u0026gt; Finally, Azure side deployment is finished. Linux VM : \u0026ldquo;UbuntuVM\u0026rdquo; shows Running and provides a Public IP Address. It has configured and deployed all necessary components (Disk, Network I/F, NSG, Public/Private IP, etc) to run our Linux VM on Azure. -\u0026gt; Using Putty Configuration and Port 22 for SSH (By default it is open. You can restrict it to only allow your Private IP to access port 22 using Network security group: \u0026ldquo;UbuntuVM-nsg\u0026rdquo; -\u0026gt; While we are on the topic, We have to allow HTTP port 80 for Web Traffic as an Inbound Port rule in the Network Security Group.\n-\u0026gt; Port 80 for Web and Port 22 for SSH is Open\n(Note : SSH is exposed to the internet and can be accessed from anywhere. For security best practices, it is NOT a recommended Nor valid practice. But for this Testing purposes, I have allowed it)\n-\u0026gt;The Private Key we downloaded is important to Login into UbuntuVM (LinuxVM) we have created. -\u0026gt;Please follow below brief article on Convert .Pem to .Ppk File Using PuTTYgen Connect Linux VM using PuTTY\n2nd Step : Login to Ubuntu , Install Apache2 -\u0026gt; Once the Putty PPK file is loaded to SSH into your Ubuntu VM\nIt will look like this.\n-\u0026gt; I have used the Public IP address of UbuntuVM and .ppk file created using Putty Gen.\n-\u0026gt; Opening it will allow us to log in to our Ubuntu Linux Server. As per,\n3rd Step : Install Apache2 to make it a WebServer -\u0026gt; Run below Command to update package\nazureuser@UbuntuVM:~$ sudo apt update -\u0026gt; Install Appache2 by Running below Command and Press \u0026ldquo;Y\u0026rdquo; to Continue\u0026hellip;\nazureuser@UbuntuVM:~$ sudo apt install apache2 -\u0026gt; Once installed. Check its active (running) status by running,\nazureuser@UbuntuVM:~$ service apache2 status -\u0026gt; After verifying APACHE2 Web Server is running. Test the WebPage by going into your Browser and Paste your Ubuntu VM\u0026rsquo;s Public IP Address. We successfully deployed our WebServer. Now its time to make it really ours. By changing the Front WebPage Index.html.\nCurrently, Public IP is showing Default Apache2 Ubuntu Default Page.\n4th Step: Let\u0026rsquo;s find the Index.html file and do some HTML Editing. -\u0026gt; use cd to go to location /var/www/html\nazureuser@UbuntuVM:~$ cd /var/www/html -\u0026gt; ls command there will list the files inside that html directory. In this case it showed us index.html file. As per, Important Process: -\u0026gt; 1. Remove existing index.html file by running **sudo rm index.html ** -\u0026gt; 2. Create new index.html file by running **sudo touch index.html ** -\u0026gt; Edit the Index.html file by opening the file in vi editor using sudo\nazureuser@UbuntuVM:/var/www/html$ sudo vi index.html -\u0026gt; vi file editor will open. where you can paste your own HTML Code or follow mine (Copy below) whichever you want to make your Display Page (index.html) for your Web Server.\n\u0026lt;/html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;My Apache Index Page!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;\u0026lt;h1\u0026gt; Welcome to my Index Page.\u0026lt;/h1\u0026gt;\u0026lt;br\u0026gt; Running out of Apache Linux Server Deployed on Azure \u0026lt;/p\u0026gt; \u0026lt;br\u0026gt; \u0026lt;h2\u0026gt; Webpage coming soon..\u0026lt;/h2\u0026gt;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt; \u0026lt;h3\u0026gt; Thanks for Stopping By\u0026lt;/h3\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Important : vi editor is tricky for first-time user. So follow below will help,\nsudo vi filename.xyz will get you in. key \u0026ldquo;i\u0026rdquo; will allow you to insert meaning allow to edit once edited , to get out of insert mode, press \u0026ldquo;esc\u0026rdquo; escape then to go out of vi editor and save the file press \u0026ldquo;:wq\u0026rdquo; + enter. You file you edited is saved successfully. Thats it.\nI know this HTML is basic and I do not even try hard. But that\u0026rsquo;s not the point of this blog. Index.html is ready and customized as per our Web Front end (under /var/www/html/index.html)\nWe are almost Done. Open the Public IP address of your VM. Now it\u0026rsquo;s a WebServer IP.\nYou will see your Edited HTML on a WebPage on any browser.\nFinal Step, Display our Webpage on Public IP Address. Hope you enjoyed the blog and follow along to learn something from it. Feel free to Let me know if you have any questions or improvements I can provide.\nI hope you like this Blog about \u0026ldquo;Apache (Ubuntu Linux) Web Server deployed on Azure\u0026rdquo;.\nThanks for your time. Follow for more Awesome Azure and AWS Content.\nRegards,\nJineshkumar Patel\n","date":"June 8, 2020","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/posts/azure/azure-blogs/host-static-website/host-your-static-website-using-apache2-on-a-linux-machine-deployed-on-azure./","summary":"How to host your static website using Apache2 on a Linux machine deployed on Azure. In this quick tutorial, We are going to learn Azure Linux VM Configuration, Few Linux VM Commands, and Apache Web Server Install and display our own Webpage on a Public IP address from anywhere.\nDeployment Components : Linux VM : Ubuntu Server 20.04 LTS Compute Resources on : **Azure ** Web Server : **Apache2 ** Web Front End : HTML Deployment Steps 1st Step: Deploy Azure Compute Resource : Linux VM (Ubuntu 20.","tags":["Azure","Cloud","StaticWebsite","Linux"],"title":"How to host your static website using Apache2 on a Linux"},{"categories":["Basic"],"contents":"How to PASS AZ-900 Azure Fundamental (Training Materials) AZ-900 Azure Fundamentals All Major Microsoft Azure Associate and Expert Level certifications need a strong Foundation and Azure Fundamentals Certification ( AZ-900 ) is providing just that.\nAZ-900 Skills measured The content of this exam was updated on November 9, 2020. Please download the exam skills outline to see what changed.\n**Describe cloud concepts (20-25%) ** -\u0026gt;Public/Private/Hybrid Cloud\n-\u0026gt;Iaas, PaaS, SaaS,\n-\u0026gt;High-Availibility, Scalability, Elasticity, Agility, DR , CapEx vs OpEx,\n-\u0026gt;Pay-As-You-Go-Price Model\n**Describe core Azure services (15-20%) ** -\u0026gt;Regions, AZ, RG, Subscriptions, Management Groups, Azure Resource Manager\n-\u0026gt;Resource Services like : Azure VM, App Service, Container Instance, Kubernetes Service\n-\u0026gt;Networking Services like : Virtual Networks, VPN Gateway, Virtual Network, Peering, Express route.\n-\u0026gt;Storage Services like : Blob, Disk, File, Table, Queue\n-\u0026gt;Database Services like : Cosmos DB, SQL DB, MySQL, PostgreSQL, SQL Instance\n**Describe core solutions and management tools on Azure (10-15%) ** Basic Understanding and definition of some Azure code Solutions and tools,\n-\u0026gt;IoT Hub, Azure Sphere, Synapse Analytics, HDInsight, HDInsight, Databricks.\n-\u0026gt;Azure Machine learning, Cognitive Services, Bot Service, Functions \u0026amp; Logic Apps\n-\u0026gt;Azure DevOps, Github, Github Actions and Azure DevTest Labs\nDescribe general security and network security features (10-15%)\n-\u0026gt;Azure Security Center, Policy Compliance, Alerts, Resource Hygiene.\n-\u0026gt;Azure Key Vault, Sentinel, Azure Deicated Hosts functionality and usage.\n-\u0026gt;Network Security Group (NSG) , Azure Firewall, Azure DDoS Protection\nDescribe identity, governance, privacy, and compliance features (20-25%)\n-\u0026gt;Authentication and Authorization\n-\u0026gt;Azure Active Directory, Conditional Access, MFS, SSO\n-\u0026gt;RBAC, Azure Policy, Users, Groups, Roles\n-\u0026gt; Cloud Adoption Framework for Azure (CAF)\nDescribes Privacy and Compliance for Security, Privacy and Data protection.\n-\u0026gt;Azure Compliance Documentation.\nDescribe Azure cost management and Service Level Agreements (10-15%)\n-\u0026gt;Azure SLA Service Level Agreement and Support Plans.\n-\u0026gt;Service LifeCycle in Azure.\nThat\u0026rsquo;s it. Don\u0026rsquo;t get Overwhelmed, as i mentioned everything mentioned above require basic knowledge and workability regarding those Azure Resources and Service definition to best understand each concept to get everything started.\nNo Deep Dive into any of those to create associate-level complex Azure Based Solution questions that require Hands-on experience. So Relex and follow along to know how to know all the above.\nFIRST and MOST Important Resource [Microsoft Learn - LEARNING PATH Part 1 to 6]\n(https://docs.microsoft.com/en-us/learn/certifications/azure-fundamentals/)\nEach part focuses on different SKILLS MEASURED listed above in Video / Documentation tutorial.\nWatch / Read through briefly. That\u0026rsquo;s enough to know its Basics, Key Points are explained in them to Introduce each part for Azure Fundamental Exam.\nDon\u0026rsquo;t worry if it does not give you any Hands-On Experience YET. We will have another resource for that NEXT, available for Free.\nMicrosoft Learn is a very good resource to know any Azure-related Topic as they have a Hands-On Tutorial / Learning Path for the detailed topics. You will know in Future more advanced topics for our Associate / Expert-Level Training.\nFor now, MS Learn Part 1 to 6 would be more than enough first step for AZ-900\nOnly Paid Course (usually 10$) I recommend for Az-900 if you want. (Not mandatory)\nAzure Az-900 Udemy Course Created by **Alan Rodrigues **(Video + Hands On) (https://www.udemy.com/course/microsoft-azure-beginners-guide/)\nI bought it when it was on Sale on Udemy, Usually for 10 $\nAnd I must tell you, It\u0026rsquo;s worth every penny of it. Must take if you can. All Free Resources (Video + Hands On) Youtube All-Star Courses\nby Adam Marczak Its a Playlist for everything AZ-900 Needs. He explains Azure Fundamentals as its a Cake Walk.\nby Andrew Brown - CEO - ExamPro this is internet Favourite One Video Course for AZ-900.\nMy thoughts \u0026ldquo;Its PPT and Lecture Base Course. Yet so much informative and fast paced. Less Hand\u0026rsquo;s On. \u0026quot;\nTake it for the love for Awesome Online Community supporting FreeCodeCamp.org\nThey are providing (Programming, Cloud related) world-class courses for free.\nKudos to them.\nI think you are Ready. But WAIT\u0026hellip;\u0026hellip;\nWhat about Hands-On Labs you can do.\nCheck out Microsoft Learning Github Repo for AZ-900 Labs\nCheck out k21Academy.com AZ-900 Labs\nFollow along all 24 Labs listed and explained how to do it with your Free Azure Account - Free Tier Creation. MUST DO if you want to excel in your Exam.\nI think you are ready now if you have followed the above path,\nRegister for the Exam and Schedule it here,\nExam AZ-900: Microsoft Azure Fundamentals ($ 99 for Normal Schedule or $ 15 if you are unemployed or affected by Covid-19)\nRevise everything or any one thing that you like covers best for you before exam.\nBONUS : this awesome PPT covers great as a revision material before AZ-900 EXAM. Click and go through a day or two before the EXAM Date.\n[AZ-900 Exam Topic PPT] (https://softwarearchitect.ca/wp-content/uploads/2019/08/AZ-900.pdf)\nMay the Cloud Be with you. GOOD LUCK !!\nI hope you like my resource material list for AZ-900 Exam and help you pass the Exam with ease. Like, Share, and Follow for better reach for anyone in need for this.\nCheck out my Other Blogs on Azure and AWS. Follow for more\u0026hellip;\nThanks for the read.\nRegards,\nJineshkumar Patel\n","date":"June 8, 2020","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/posts/azure/azure-blogs/pass-az900/how-to-pass-az-900-azure-fundamental/","summary":"How to PASS AZ-900 Azure Fundamental (Training Materials) AZ-900 Azure Fundamentals All Major Microsoft Azure Associate and Expert Level certifications need a strong Foundation and Azure Fundamentals Certification ( AZ-900 ) is providing just that.\nAZ-900 Skills measured The content of this exam was updated on November 9, 2020. Please download the exam skills outline to see what changed.\n**Describe cloud concepts (20-25%) ** -\u0026gt;Public/Private/Hybrid Cloud\n-\u0026gt;Iaas, PaaS, SaaS,\n-\u0026gt;High-Availibility, Scalability, Elasticity, Agility, DR , CapEx vs OpEx,","tags":["Azure","Cloud","Fundamental","Study"],"title":"How to PASS AZ-900 Azure Fundamental"},{"categories":["Basic"],"contents":"CloudCubes-by-Jinesh All my Hashnode Blogs\nMy Blog Link\nAWS Blogs\nAzure Blogs\n","date":"June 8, 2020","hero":"/posts/introduction/hero.svg","permalink":"https://jineshkumar1.github.io/posts/introduction/","summary":"CloudCubes-by-Jinesh All my Hashnode Blogs\nMy Blog Link\nAWS Blogs\nAzure Blogs","tags":["Basic","Multi-lingual"],"title":"Introduction"},{"categories":["Basic"],"contents":"Introduction to ARM Templates (AZURE IaC DevOps)\n**ARM (Azure Resource Manager)Templates ** are what really gives us the ability to roll out Azure “Infrastructure as code”.\nThey are project\u0026rsquo;s infrastructure as Code in a declarative and reusable Template. A way to declare the objects/resources you want, the types, its names, and properties in a JSON ( JavaScript Object Notation ) file format which can be checked into source control and managed like any other code file. The templates can be versioned and saved in the same source control repository as your development project.\nThe advantages to Infrastructure as a Code are:\nConsistent configurations, Repeatable results. Improved scalability and Faster deployments Better traceability (Tracked deployments) Integration with CI/CD Built-in validation and Testing How ARM Templates configured ?\nThe template uses declarative syntax.\nThe declarative syntax is a way of building the structure and elements that outline what resources will look like without describing its control flow. ARM templates allow you to declare what you intend to deploy without having to write the sequence of programming commands to create it. In an ARM template, you specify the resources and the properties for those resources. Then Azure Resource Manager uses that information to deploy the resources in an organized and consistent manner. NOTE : Resource Manager orchestrates the deployment of the resources so they\u0026rsquo;re created in the correct order. When possible, resources will also be created in parallel, so ARM template deployments finish faster than scripted deployments. **What is ARM Template JSON File Structure? **\nThe template files are made up of the following elements:\nschema: A required section that defines the location of the JSON schema file that describes the structure of JSON data. The version number you use depends on the scope of the deployment and your JSON editor.\ncontentVersion: A required section that defines the version of your template (such as 1.0.0.0). You can use this value to document significant changes in your template to ensure you\u0026rsquo;re deploying the right template.\n**apiProfile: **An optional section that defines a collection of API versions for resource types. You can use this value to avoid having to specify API versions for each resource in the template.\nparameters: An optional section where you define values that are provided during deployment. These values can be provided by a parameter file, by command-line parameters, or in the Azure portal.\nvariables: An optional section where you define values that are used to simplify template language expressions.\nfunctions: An optional section where you can define user-defined functions that are available within the template. User-defined functions can simplify your template when complicated expressions are used repeatedly in your template.\nresources: A required section that defines the actual items you want to deploy or update in a resource group or a subscription.\noutput: An optional section where you specify the values that will be returned at the end of the deployment.\n**File structure of a ARM Template .JSON file **\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.1\u0026#34;, \u0026#34;apiProfile\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;parameters\u0026#34;: { }, \u0026#34;variables\u0026#34;: { }, \u0026#34;functions\u0026#34;: [ ], \u0026#34;resources\u0026#34;: [ ], \u0026#34;outputs\u0026#34;: { } } To be honest, It is very tedious and error pron way to create an ARM template from scratch. But Don\u0026rsquo;t worry, Azure has us covered.\nVisual Studio Extension to the rescue.\nThe Azure Resource Manager (ARM) Tools for Visual Studio Code provides language support, resource snippets, and resource auto-completion to help you create and validate Azure Resource Manager templates.\nAlso, Azure Portal has a quick and easy way to generate ARM Templates from basic configuration created using Portal. You do not need to deploy the resources using azure portal. Just Download the Template as it has created while configuring the basic resource creation process on the azure portal. And then we can edit, add, remove, customize, and Improve the template to make it Deployment ready for us as we desire.\nI feel that is the quick and easy way to create new ARM Templates.\nFor Example,\nIn above Screenshot, Shows a \u0026ldquo;Storage Account\u0026rdquo; Resouce creation (It can be any azure resource) using Azure Portal.\nSelect Review + create on the bottom of the screen. DO NOT CLICK Create in the next step.\nSelect Download a template for automation on the bottom of the screen. The portal shows the generated template:\nSelect Download Template as per,\nThe downloaded JSON file can be open in VS Code or a normal Notepad Application.\nIf you notice there are parameters defined like \u0026ldquo;location\u0026rdquo; , \u0026ldquo;StorageAccounrName\u0026rdquo; , \u0026ldquo;AccountType\u0026rdquo;, \u0026ldquo;Kind\u0026rdquo;, \u0026ldquo;accessTier\u0026rdquo;, \u0026ldquo;minimumTLSVersion\u0026rdquo; \u0026ldquo;AllowBlob/SharedKeyAccess\u0026rdquo; and its Values as per,\nYou can edit and Customize the \u0026ldquo;Value\u0026rdquo; of each parameter as per your desired configuration to make it a Custom ARM Template.\nOnce Desired Configuration on Custom ARM Template is set by the Edit of the JSON file. You can Deploy the Custom Template directly back to Azure Portal. Check below, Or How Cool !! Right?\nAnd this is just a basic example I give here to understand the concept of ARM Template.\nYou can make a custom ARM Template with much more using almost all Azure Resources and make it deploy ready and add it to your code repository for version control and CI/CD deployments.\nOr use Azure CLI Create Resource group\naz deployment group create \u0026ndash;name blanktemplate \u0026ndash;resource-group myResourceGroup \u0026ndash;template-file $templateFile\nDeploy your created Template.json file\n$templateFile=\u0026quot;{provide-the-path-to-the-template-file}\u0026quot; az deployment group create \u0026ndash;name blanktemplate \u0026ndash;resource-group myResourceGroup \u0026ndash;template-file $templateFile\nAnd verify from Portal of our successful ARM teplate deployment\nResource to Learn more :\nMicrosoft Azure Github Repo for multiple examples of ARM Templates AzureStack-QuickStart-Templates\nFeel free to try and Let me know if you have any questions or improvements I can provide.\nI hope you like the Blog: Introduction to ARM Templates.\nThanks for the read. Follow for more Awesome Azure and AWS Content.\nRegards,\nJineshkumar Patel\n","date":"June 8, 2020","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/posts/azure/azure-blogs/intro-to-arm-template/introduction-to-azure-arm-templates/","summary":"Introduction to ARM Templates (AZURE IaC DevOps)\n**ARM (Azure Resource Manager)Templates ** are what really gives us the ability to roll out Azure “Infrastructure as code”.\nThey are project\u0026rsquo;s infrastructure as Code in a declarative and reusable Template. A way to declare the objects/resources you want, the types, its names, and properties in a JSON ( JavaScript Object Notation ) file format which can be checked into source control and managed like any other code file.","tags":["Azure","Cloud","ARM","IaC"],"title":"Introduction to ARM Templates"},{"categories":null,"contents":"This is a sample post intended to test the followings:\nA different post author. Table of contents. Markdown content rendering. Math rendering. Emoji rendering. Markdown Syntax Rendering Headings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Inline Markdown In Table italics bold strikethrough code Code Blocks Code block with backticks html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nMath Rendering Block math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\nEmoji Rendering 🙈 🙈 🙉 🙉 🙊 🙊\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"June 8, 2020","hero":"/posts/markdown-sample/hero.svg","permalink":"https://jineshkumar1.github.io/posts/markdown-sample/","summary":"This is a sample post intended to test the followings:\nA different post author. Table of contents. Markdown content rendering. Math rendering. Emoji rendering. Markdown Syntax Rendering Headings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur?","tags":null,"title":"Markdown Samples"},{"categories":null,"contents":"This is a sample post intended to test the followings:\nDefault hero image. Different shortcodes. Alert The following alerts are available in this theme.\nThis is sample alert with type=\u0026quot;success\u0026quot;. This is sample alert with type=\u0026quot;danger\u0026quot;. This is sample alert with type=\u0026quot;warning\u0026quot;. This is sample alert with type=\u0026quot;info\u0026quot;. This is sample alert with type=\u0026quot;dark\u0026quot;. This is sample alert with type=\u0026quot;primary\u0026quot;. This is sample alert with type=\u0026quot;secondary\u0026quot;. Image A sample image without any attribute. A sample image with height and width attributes. A center aligned image with height and width attributes. A image with float attribute. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras egestas lectus sed leo ultricies ultricies. Praesent tellus risus, eleifend vel efficitur ac, venenatis sit amet sem. Ut ut egestas erat. Fusce ut leo turpis. Morbi consectetur sed lacus vitae vehicula. Cras gravida turpis id eleifend volutpat. Suspendisse nec ipsum eu erat finibus dictum. Morbi volutpat nulla purus, vel maximus ex molestie id. Nullam posuere est urna, at fringilla eros venenatis quis.\nFusce vulputate dolor augue, ut porta sapien fringilla nec. Vivamus commodo erat felis, a sodales lectus finibus nec. In a pulvinar orci. Maecenas suscipit eget lorem non pretium. Nulla aliquam a augue nec blandit. Curabitur ac urna iaculis, ornare ligula nec, placerat nulla. Maecenas aliquam nisi vitae tempus vulputate.\nSplit This theme support splitting the page into as many columns as you wish.\nTwo column split Left Column Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras egestas lectus sed leo ultricies ultricies. Right Column Fusce ut leo turpis. Morbi consectetur sed lacus vitae vehicula. Cras gravida turpis id eleifend volutpat. Three column split Left Column Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras egestas lectus sed leo ultricies ultricies. Middle Column Aenean dignissim dictum ex. Donec a nunc vel nibh placerat interdum. Right Column Fusce ut leo turpis. Morbi consectetur sed lacus vitae vehicula. Cras gravida turpis id eleifend volutpat. Vertical Space Give vertical space between two lines.\nThis is line one. This is line two. It should have 4rem vertical space with previous line.\nVideo Video by Rahul Sharma from Pexels.\nMermaid Here, are few example of mermaid shortcode.\nGraph:\ngraph LR; A[Hard edge] --\u003e|Link text| B(Round edge) B --\u003e C{Decision} C --\u003e|One| D[Result one] C --\u003e|Two| E[Result two] Sequence Diagram:\nsequenceDiagram participant Alice participant Bob Alice-\u003e\u003eJohn: Hello John, how are you? loop Healthcheck John-\u003e\u003eJohn: Fight against hypochondria end Note right of John: Rational thoughts prevail! John--\u003e\u003eAlice: Great! John-\u003e\u003eBob: How about you? Bob--\u003e\u003eJohn: Jolly good! Gantt diagram:\ngantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d Class Diagram:\nclassDiagram Class01 \u003c|-- AveryLongClass : Cool Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 --\u003e C2 : Where am i? Class09 --* C3 Class09 --|\u003e Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08 \u003c--\u003e C2: Cool label Git Graph:\ngitGraph: options { \"nodeSpacing\": 150, \"nodeRadius\": 10 } end commit branch newbranch checkout newbranch commit commit checkout master commit commit merge newbranch ER Diagram:\nerDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses Gist ","date":"June 8, 2020","hero":"/posts/shortcodes/boat.jpg","permalink":"https://jineshkumar1.github.io/posts/shortcodes/","summary":"This is a sample post intended to test the followings:\nDefault hero image. Different shortcodes. Alert The following alerts are available in this theme.\nThis is sample alert with type=\u0026quot;success\u0026quot;. This is sample alert with type=\u0026quot;danger\u0026quot;. This is sample alert with type=\u0026quot;warning\u0026quot;. This is sample alert with type=\u0026quot;info\u0026quot;. This is sample alert with type=\u0026quot;dark\u0026quot;. This is sample alert with type=\u0026quot;primary\u0026quot;. This is sample alert with type=\u0026quot;secondary\u0026quot;. Image A sample image without any attribute.","tags":null,"title":"Shortcodes Samples"},{"categories":["Basic"],"contents":"How-To SYNC your On-Premise AD with AZURE AD (Hands-On) In this Tutorial Blog, We will integrate a Single Forest On-Premise Active Directory (AD) with our Azure Active Directory(AAD) using\nAzure AD Connect \u0026gt; \u0026ldquo;Azure AD cloud sync\u0026rdquo;\nWhat is Azure AD Connect cloud sync? Azure AD Connect Cloud Sync is a cloud service alternative to Azure AD Connect software. The organization deploys one or more lightweight agents in their on-premises environment to bridge AD and Azure AD. The configuration is done in the cloud.\nPrerequisites Global administrator account on your Azure AD. Tenant in Azure Active Directory. On-Premise AD Server Administrator Access. My Set up\nServer 2016 running from VirtualBox. Conside it On-Prem Server. AD DS Role Installed (AD Server.)(Local Forest Domain is JP.local) Azure Portal Global Administrator access. Step by step guide to Follow Create Azure AD and Create a Tanent\nTanent Type : Azure Active Directory Configure the Organization Name and Domain Name. Review+Create Download Azure AD cloud sync Agent and Move it to your Local On Premise Active Directory Server\n**Installing AADConnectProvisioningAgentSetup.exe on On Prem AD Server. ** Connect/Authenticate Login to Azure AD using Global Admin Account created for Azure AD Directory. Configure Service Account Connect On Prem Active Directory Domain (Mine is JP.local)\nAgent Install Configuration Confirm Page.\nActive Directory Configuration to Local Domain (JP.LOCAL) Azure Active Directory Global Administrator Login configuration.\nYour agent Installation and Configuration is complete. Provision on Azure AD Portal. Azure portal agent verification by \u0026gt; Azure AD\u0026gt; Azure AD Connect Cloud Sync \u0026gt; Review All Agents \u0026gt;It shows Active for my AD Machine. Also, verify on On-Premise AD Server Services where \u0026ldquo;Azure AD Connect Agent\u0026rdquo; Services are running,\nSo far so good. Verified. Status: Active.\nConfigure Azure AD Connect cloud sync Portal \u0026gt; Azure AD \u0026gt; Azure AD Connect \u0026gt; Manage Azure AD cloud sync \u0026gt; New Configuration.\nIt will automatically filled recently installed AD Connect Cloud Sync Agent Domain (Mine is JP.local) Next will allow to edit this provisioning configuration which asks Configuration Scope for Domain or if you want to use any Filter for scope for sync. Also Validate , Notification settings and deploy.\nGreat ! Saved and My AD Provisioning for JP.local domain is showing **\u0026ldquo;HEALTHY\u0026rdquo; STATUS ** Verify users are created and synchronization is occurring\nFor this Test, I created 4 Users (First, Second, Third, Fourth) And Wooooalllaaaa!! It all synced in few Seconds to my Azure AD under all Users. Note : Directory Synced: YES for local AD Synced Users.\nManaging Users from Microsoft 365 Admin Center I could have also assigned an Administrator Role and have New User signed in into MS 365 Admin Center to manage AD Administration.\nWe have now successfully configured a hybrid identity environment between On-Prem AD and Azure AD using Azure AD Connect cloud sync. Hope you followed up with this hands-on tutorial. Thank you for the read. I appreciate your time. Let me know if you have any questions or queries during following along with the steps.\nFollow for more Azure and AWS Content.\nRegards,\nJineshkumar Patel\n","date":"June 8, 2020","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/posts/azure/azure-blogs/sync-onprem-ad-to-azuread/how-to-sync-your-on-premise-ad-with-azure-ad-hands-on/","summary":"How-To SYNC your On-Premise AD with AZURE AD (Hands-On) In this Tutorial Blog, We will integrate a Single Forest On-Premise Active Directory (AD) with our Azure Active Directory(AAD) using\nAzure AD Connect \u0026gt; \u0026ldquo;Azure AD cloud sync\u0026rdquo;\nWhat is Azure AD Connect cloud sync? Azure AD Connect Cloud Sync is a cloud service alternative to Azure AD Connect software. The organization deploys one or more lightweight agents in their on-premises environment to bridge AD and Azure AD.","tags":["Azure","Cloud","AzureAD","Identity"],"title":"SYNC your On-Premise AD  with AZURE AD"},{"categories":["Basic"],"contents":"Veeam Backup for AZURE - Deployment on Azure (Native Azure Backup - SaaS) Advantages of Veeam Backup for Azure,\n**Your Azure Backup Infrastructure looks like this, ** **Backup Appliance **\nThe backup appliance is a Linux-based Azure VM where Veeam Backup for Microsoft Azure is installed.\nBackup Appliance will run Backup Service, Config. DB, WebUI, Update Service, REST API Service. Backup Repositories A backup repository is a folder in a blob container where Veeam Backup for Microsoft Azure stores backups of Azure VMs Worker Instances\nA worker instance is an auxiliary Linux-based virtual machine that is responsible for the interaction between the backup appliance and other components of the Veeam Backup for Microsoft Azure infrastructure. Worker instances process the backup workload and distribute backup traffic when transferring data to backup repositories. Deployment Of \u0026ldquo;Veeam Backup for Microsoft Azure\u0026rdquo; Free Edition Note: For the Demo Purposes, I am selecting the minimum resources allocation required to get started with Veeam Backup for Azure.\nFind \u0026ldquo;Veeam Backup for Microsoft Azure Free Edition\u0026rdquo; on Azure Marketplace\nCreate a Virtual Machine: Select Username and Password. I selected Standard B2s (2vCPU, 4GB RAM)\nVM Disks Settings\nNetworking\nAll other Settings as Default for minimum Cost. It comes down to, Hit \u0026ldquo;Create\u0026rdquo;\nYour VM for \u0026ldquo;Veeam Backup for Azure\u0026rdquo; is Deployed.\nand below resources get created in your Azure Environment. Go to VeeamAzure VM \u0026gt; Copy the Public IP Address and Open WebUI using https://Your.Public.IP.Address (it only works with https)\nTHIS IS YOUR WEBUI (Backup Appliance), Login with the Username and Password Set while creating a Virtual Machine. Accept the \u0026ldquo;EULA\u0026rdquo; (No other Choice\u0026hellip;) And you are in the Configuration Phase of your Veeam Backup for Azure. THREE things \u0026ldquo;TO DO\u0026rdquo; here,\n1\u0026gt; Add Microsoft Azure Connection\n2\u0026gt; Add Repository\n3\u0026gt; Create your first Backup Policy Add Azure Account and Authenticate and Finish. Add Repository \u0026gt; Select New / Existing Container (Blob) Storage and Folder. Then Select Storage Tier and Encryption. IMPORTANT: Add Backup Policy (Backup Job Configuration and its Schedule settings)\nVeeam allows a vast range of features for different kinds of workload including \u0026ldquo;ApplicationAware processing\u0026rdquo; (Application-aware Snapshots), Guest Scripting, Schedule, Cost Estimation inbuilt.\nOne Cool Feature to mention : To avoid downtime, high-availability systems may perform the backup on a snapshot—a read-only copy of the data set frozen at a point in time—and allow applications to continue writing to their data. Once the initial snapshot is taken of a data set, subsequent snapshots copy the changed data only and use a system of pointers to reference the initial snapshot. This method of pointer-based snapshots consumes less disk capacity than if the data set was repeatedly cloned.\nSchedule Options (ALL Configuration as per your requirements)\n**Cost Estimation in built. Cool huh ?! ** (But remember, its an estimate)\nAnd at the End, SUMMARY of your Backup Policy.\n(Here I am backing up two of my important Azure Projects At the Resource Group Level. ) Final Overview of the WebUI for\n** \u0026ldquo;Veeam Backup for Microsoft Azure\u0026rdquo;. **\nVeeam Backup for Microsoft Azure is a solution developed for protection and disaster recovery tasks for Microsoft Azure environments. With Veeam Backup for Microsoft Azure, you can perform the following operations:\nCreate image-level backups and cloud-native snapshots of Azure VMs. Keep the backed-up data in cost-effective and long-term Microsoft Azure storage accounts. Restore entire Azure VMs, individual virtual disks, and guest OS files and folders. It has many more key features that help Backup and Restores your Azure workload, faster, and with easy configuration. Veeam is continuously Leading the Industry in \u0026ldquo;Enterprise Backup and Recovery Software Solutions\u0026rdquo; and products like this will help Customers adopt Public Cloud Environment easily and rely more on their Disaster Recovery Solutions in Cloud.\nFeel free to follow along and Let me know if you have any questions or improvements I can provide.\nI hope you like the Blog: \u0026ldquo;Veeam Backup for Azure\u0026rdquo;.\nThanks for the read. Follow for more Awesome Azure and AWS Content.\nRegards,\nJineshkumar Patel\n","date":"June 8, 2020","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/posts/azure/azure-blogs/veeam-backup-for-azure/veeam-backup-for-azure-deployment-on-azure/","summary":"Veeam Backup for AZURE - Deployment on Azure (Native Azure Backup - SaaS) Advantages of Veeam Backup for Azure,\n**Your Azure Backup Infrastructure looks like this, ** **Backup Appliance **\nThe backup appliance is a Linux-based Azure VM where Veeam Backup for Microsoft Azure is installed.\nBackup Appliance will run Backup Service, Config. DB, WebUI, Update Service, REST API Service. Backup Repositories A backup repository is a folder in a blob container where Veeam Backup for Microsoft Azure stores backups of Azure VMs Worker Instances","tags":["Azure","Cloud","Storage","Azure Storage","veam Backup"],"title":"Veeam Backup for Azure"},{"categories":null,"contents":"","date":"June 8, 2020","hero":"/images/default-hero.jpg","permalink":"https://jineshkumar1.github.io/posts/aws/aws-blogs/analytics-and-commands/analytics-and-comments/","summary":"","tags":null,"title":"Analytics and Comments"}]